{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Hidden Markov state models (HMMs)\n",
    "In this notebook, we will learn about hidden Markov state models and how to use them to deal with bad discretization. We further explain how to obtain a coarse model based on an initial MSM analysis.\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons Licence\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" title='This work is licensed under a Creative Commons Attribution 4.0 International License.' align=\"right\"/></a>\n",
    "\n",
    "Maintainers: [@cwehmeyer](https://github.com/cwehmeyer), [@marscher](https://github.com/marscher), [@thempel](https://github.com/thempel), [@psolsson](https://github.com/psolsson)\n",
    "\n",
    "Remember, to\n",
    "- run the currently highlighted cell, hold <kbd>&#x21E7; Shift</kbd> and press <kbd>&#x23ce; Enter</kbd>;\n",
    "- get help for a specific function, place the cursor within the function's brackets, hold <kbd>&#x21E7; Shift</kbd>, and press <kbd>&#x21E5; Tab</kbd>;\n",
    "- find the full documentation at [PyEMMA.org](http://www.pyemma.org).\n",
    "\n",
    "---\n",
    "\n",
    "When estimating a regular MSM, we assume that the dynamics between microstates defined by our clustering algorithm is Markovian. Hidden Markov models (HMMs), in comparison, only make this assumption in the space of so-called hidden states that have not been directly observed. This means that instead of finding a maximum likelihood (ML) transition matrix between microstates, an ML transition matrix is estimated between hidden states. In the same step, the probability of a microstate to belong to a certain hidden state is estimated. Please note that the `PyEMMA` implementation internally initiates the estimation with a regular MSM with PCCA++.\n",
    "\n",
    "There are two major use-cases for HMMs in `PyEMMA`:\n",
    "- HMMs are more robust to poor space discretization and can be used to overcome difficult clustering situations\n",
    "- HMMs offer a coarse graining into metastable (hidden) states\n",
    "\n",
    "In this notebook, we will demonstrate how to estimate HMMs and how they behave in comparison to MSMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mdshare\n",
    "import pyemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: preprocessed, two-dimensional data (toy model)\n",
    "\n",
    "In this example, we are going to demonstrate the robustness of HMMs against poor discretization and show some of its properties. We start by loading the two-dimensional as well as the true discrete trajectory from an archive using `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = mdshare.fetch('hmm-doublewell-2d-100k.npz', working_directory='data')\n",
    "with np.load(file) as fh:\n",
    "    data = fh['trajectory']\n",
    "    good_dtraj = fh['discrete_trajectory']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now estimate a reference (regular) MSM from the well-discretized data which is shown in the next panel (left). We include an implied timescales plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "pyemma.plots.plot_state_map(*data.T, good_dtraj, ax=axes[0])\n",
    "axes[0].scatter(*np.asarray([[0, -1], [0, 1]]).T, s=15, c='C1')\n",
    "\n",
    "axes[0].set_xlabel('$x$')\n",
    "axes[0].set_xlim(-4, 4)\n",
    "axes[0].set_ylim(-4, 4)\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].set_ylabel('$y$')\n",
    "axes[0].set_title('discretization')\n",
    "\n",
    "lags = [i for i in range(1, 10)]\n",
    "pyemma.plots.plot_implied_timescales(\n",
    "    pyemma.msm.its(good_dtraj, lags=lags, errors='bayes'), ylog=False, ax=axes[1])\n",
    "axes[1].set_title('MSM with good discretization')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that for this very good discretization, the implied timescales are converged  from lagtime $1$. We continue to build an MSM object and perform the Chapman-Kolmogorov test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_msm = pyemma.msm.estimate_markov_model(good_dtraj, lag=1)\n",
    "pyemma.plots.plot_cktest(reference_msm.cktest(2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chapman-Kolmogorov test shows excellent agreement between higher lagtime estimation and model prediction. We thus take this model as a reference.\n",
    "\n",
    "Let's now deliberately choose a very bad discretization..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_clustercenters = np.asarray([[-2.5, -1.4], \n",
    "                                  [0.3, 1.2], \n",
    "                                  [2.7, -0.6]])\n",
    "poor_dtraj = pyemma.coordinates.assign_to_centers(data, centers=poor_clustercenters)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and repeat the ITS estimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "pyemma.plots.plot_state_map(*data.T, poor_dtraj, ax=axes[0])\n",
    "axes[0].scatter(*poor_clustercenters.T, s=15, c='C1')\n",
    "\n",
    "axes[0].set_xlabel('$x$')\n",
    "axes[0].set_xlim(-4, 4)\n",
    "axes[0].set_ylim(-4, 4)\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].set_ylabel('$y$')\n",
    "axes[0].set_title('discretization')\n",
    "\n",
    "pyemma.plots.plot_implied_timescales(\n",
    "    pyemma.msm.its(poor_dtraj, lags=lags, errors='bayes'), ylog=False, ax=axes[1])\n",
    "axes[1].set_title('MSM with poor discretization')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the discretization is very poor and does not mirror the wells of the double-well potential anymore. All three discrete states include data points from the two metastable regions (left panel) and, as the right panel shows, this discretization error cannot be fixed by using a large lagtime for a regular MSM estimation. Thus, the MSM clearly is not able to resolve the slow process connecting the two basins any more. We do not see any ITS above the lag time horizon and, hence, cannot estimate any MSM with this discretization.\n",
    "\n",
    "\n",
    "Let us now repeat both estimations using hidden Markov state models instead of regular MSMs. We begin with the implied timescale convergence using the `pyemma.msm.timescales_hmsm()` function and two hidden states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "its_hmm_poor = pyemma.msm.timescales_hmsm(poor_dtraj, 2, lags=lags, errors='bayes')\n",
    "its_hmm_good = pyemma.msm.timescales_hmsm(good_dtraj, 2, lags=lags, errors='bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We go on visualizing the results as with regular MSM implied timescales and include, as a dotted line,  the result from our previously estimated reference MSM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "pyemma.plots.plot_implied_timescales(\n",
    "    its_hmm_poor,\n",
    "    ylog=False, ax=axes[0])\n",
    "pyemma.plots.plot_implied_timescales(\n",
    "    its_hmm_good,\n",
    "    ylog=False, ax=axes[1])\n",
    "axes[0].set_title('HMM with poor discretization')\n",
    "axes[1].set_title('HMM with good discretization')\n",
    "\n",
    "for n, ax in enumerate(axes.flat):\n",
    "    ax.set_ylim(-0.5, 12.5)\n",
    "    ax.hlines(reference_msm.timescales()[0], *ax.get_xlim(), linestyle=':', label='reference MSM' if n == 0 else None)\n",
    "fig.legend(loc=9)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to a regular MSM, both discretizations give us converged implied timescales from the very start (lagtime $1$ step). As the HMM computes the implied timescales of a process between two hidden states, we do not assume Markovianity in the original state space. Thus, the deliberate discretization error we made is compensated by the algorithm, making it robust against poor clustering.\n",
    "\n",
    "In order to validate this claim, we estimate HMMs using both discretizations at lagtime $1$ step and two hidden states..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "poor_hmm = pyemma.msm.estimate_hidden_markov_model(poor_dtraj, 2, lag=1)\n",
    "good_hmm = pyemma.msm.estimate_hidden_markov_model(good_dtraj, 2, lag=1)\n",
    "\n",
    "print('MSM (ref):  1. implied timescale = {:.2f} steps'.format(reference_msm.timescales()[0]))\n",
    "print('HMM (poor): 1. implied timescale = {:.2f} steps'.format(poor_hmm.timescales()[0]))\n",
    "print('HMM (good): 1. implied timescale = {:.2f} steps'.format(good_hmm.timescales()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and obtain nearly identical estimates for the first implied timescale that agree with the reference MSM.\n",
    "\n",
    "We observe that HMMs, unlike MSMs, seem to be somewhat resistant to discretization errors.\n",
    "\n",
    "Regarding the CK test, we again see that the `poor_hmm`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemma.plots.plot_cktest(poor_hmm.cktest(2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the `good_hmm`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemma.plots.plot_cktest(good_hmm.cktest(2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... are in perfect agreement and the Chapman-Kolmogorov test has passed.\n",
    "\n",
    "The HMM, even for the poor discretization, has learned an assignment between the microstates to the hidden states that reflects the true dynamics. We can extract this information from the HMM object using its `hmm.hidden_state_probabilities` property. It contains the probabilities for each microstate to be in a given hidden state over time, for each trajectory (which is why we have to take the $0$-th element from this list). \n",
    "\n",
    "Please note that there is also a time-independent property that contains the observation probabilities of a hidden state as a function of the microstates. It is stored in `hmm.observation_probabilities` but not demonstrated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(10, 4), sharex=True)\n",
    "axes[0].plot(data[:100, 1], 'r.-', label='cont. traj')\n",
    "axes[0].step(np.arange(100), poor_clustercenters[poor_dtraj[:100], 0], label='discrete traj', linewidth=1, c='k')\n",
    "axes[0].set_ylabel('y coordinate \\ncluster center')\n",
    "\n",
    "for n, hidden_state_probability_traj in enumerate(poor_hmm.hidden_state_probabilities[0].T):\n",
    "    axes[1].plot(hidden_state_probability_traj[:100], '.-', label='P(state {})'.format(n+1))\n",
    "    \n",
    "axes[1].set_ylabel('hidden state prob')\n",
    "fig.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see in the trajectory excerpt of the first $100$ steps, the microstate assignment probabilities to hidden states over time reflect the actual dynamics very well, even though the discrete trajectory has basically lost notion of the original double-well basins. Let's now plot the hidden state probabilities in the original space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    pyemma.plots.plot_contour(\n",
    "        *data.T,\n",
    "        poor_hmm.hidden_state_probabilities[0][:, i], # index 0: only 1 trajectory\n",
    "        ax=ax,\n",
    "        cmap='afmhot_r', \n",
    "        mask=True,\n",
    "        cbar_label='P(state {})'.format(i+1))\n",
    "    ax.set_xlabel('$y')\n",
    "axes[0].set_ylabel('$x$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we find some artifacts of the initial clustering, the HMM has basically overcome the poor discretization and found hidden states that, with high certainty, mirror the original double-well basins. Let's finish this example by comparing the hidden state trajectories with the discrete trajectories that were used for estimating the reference MSM (the \"true\" discrete trajectories)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('HMM (good): Hidden state trajectory consistency: '\n",
    "      '{:.3f}'.format(sum(good_hmm.hidden_state_trajectories[0] == good_dtraj)/good_dtraj.shape[0]))\n",
    "print('HMM (poor): Hidden state trajectory consistency: '\n",
    "      '{:.3f}'.format(sum(poor_hmm.hidden_state_trajectories[0] == good_dtraj)/good_dtraj.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the stationary distributions of both HMMs to the reference MSM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSM (ref):  stationary distribution = {}'.format(np.round(reference_msm.pi, 4)))\n",
    "print('HMM (poor): stationary distribution = {}'.format(np.round(poor_hmm.pi, 4)))\n",
    "print('HMM (good): stationary distribution = {}'.format(np.round(good_hmm.pi, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that even with a very poor discretization, HMMs are capable of recovering the kinetics of the underlying process with very little error. \n",
    "\n",
    "## Case 2: low-dimensional molecular dynamics data (alanine dipeptide)\n",
    "We are now illustrating a typical use case of hidden markov state models: estimating an MSM that is used as a heuristics for the number of slow processes or hidden states, and estimating an HMM (to resolve faster processes than an MSM).\n",
    "\n",
    "We fetch the alanine dipeptide data set, load the backbone torsions into memory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb = mdshare.fetch('alanine-dipeptide-nowater.pdb', working_directory='data')\n",
    "files = mdshare.fetch('alanine-dipeptide-*-250ns-nowater.xtc', working_directory='data')\n",
    "\n",
    "feat = pyemma.coordinates.featurizer(pdb)\n",
    "feat.add_backbone_torsions()\n",
    "data = pyemma.coordinates.load(files, features=feat)\n",
    "data_concatenated = np.concatenate(data)\n",
    "\n",
    "cluster = pyemma.coordinates.cluster_kmeans(data, k=75, max_iter=50, stride=10)\n",
    "dtrajs_concatenated = np.concatenate(cluster.dtrajs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... discretize the full space using $k$-means clustering, visualize the marginal and joint distributions of both components as well as the cluster centers, and show the ITS convergence to help selecting a suitable lag time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "its = pyemma.msm.its(\n",
    "    cluster.dtrajs, lags=[1, 2, 5, 10, 20, 50], nits=4, errors='bayes')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "pyemma.plots.plot_feature_histograms(data_concatenated, feature_labels=['$\\Phi$', '$\\Psi$'], ax=axes[0])\n",
    "pyemma.plots.plot_density(*data_concatenated.T, ax=axes[1], cbar=False, alpha=0.3)\n",
    "axes[1].scatter(*cluster.clustercenters.T, s=15, c='C1')\n",
    "axes[1].set_xlabel('$\\Phi$')\n",
    "axes[1].set_ylabel('$\\Psi$')\n",
    "pyemma.plots.plot_implied_timescales(its, ax=axes[2], units='ps')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the implied timescale convergence plot, we choose a lagtime of $10$ steps. We further find $3$ slow processes in the implied timescales plot, meaning that we can expect $4$ metastable sets or hidden states. First, we estimate a Bayesian MSM, and show the results of a CK test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_msm = pyemma.msm.bayesian_markov_model(cluster.dtrajs, lag=10, dt_traj='1 ps')\n",
    "\n",
    "nstates = 4\n",
    "pyemma.plots.plot_cktest(bayesian_msm.cktest(nstates), units='ps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have a (bayesian) MSM with $75$ discrete states and basic validation. To obtain an HMM with only four states (the number for which we have validated our MSM), we compute the implied timescales for HMMs with this number of hidden states. \n",
    "\n",
    "We repeat the ITS convergence analysis using (bayesian) HMMs and small lagtimes for a $4$-state HMM. For demonstration purposes, we add the same analysis with a $6$-state HMM to visualize what happens if the number of states is not as clear as in this example: \n",
    "\n",
    "**NOTE:** Executing the following cell might take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "pyemma.plots.plot_implied_timescales(\n",
    "    pyemma.msm.timescales_hmsm(\n",
    "        cluster.dtrajs, 4, lags=[1, 2, 3, 4, 5, 6], errors='bayes', nsamples=50),\n",
    "    ax=axes[0], units='ps')\n",
    "pyemma.plots.plot_implied_timescales(\n",
    "    pyemma.msm.timescales_hmsm(\n",
    "        cluster.dtrajs, 6, lags=[1, 2, 3, 4], errors='bayes', nsamples=50),\n",
    "    ax=axes[1], units='ps')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left panel shows that an HMM with four hidden states yields converged implied timescales from lagtime $1$.\n",
    "\n",
    "The right panel, however, shows that an HMM with six hidden states and lagtime $1$ can resolve two additional processes.\n",
    "\n",
    "Let us follow up on this and perform a CK test for a four state HMM at lagtime $1$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_4 = pyemma.msm.bayesian_hidden_markov_model(cluster.dtrajs, 4, lag=1, dt_traj='1 ps', nsamples=50)\n",
    "pyemma.plots.plot_cktest(hmm_4.cktest(mlags=6), units='ps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and than the six state HMM at laggtime $1$ (we use `mlags=2` because we would loose the two fast processes at lagtimes $\\geq3$):\n",
    "\n",
    "**Note:** Executing the following cell might take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_6 = pyemma.msm.bayesian_hidden_markov_model(cluster.dtrajs, 6, lag=1, dt_traj='1 ps', nsamples=50)\n",
    "pyemma.plots.plot_cktest(hmm_6.cktest(mlags=4), units='ps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, the CK test is passed.\n",
    "\n",
    "If we now compare both metastable membership plots..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n",
    "for hmm, ax in zip([hmm_4, hmm_6], axes.flat):\n",
    "    pyemma.plots.plot_state_map(\n",
    "        *data_concatenated.T,\n",
    "        hmm.metastable_assignments[dtrajs_concatenated], \n",
    "        ax=ax)\n",
    "    ax.set_title('HMM with {} hidden states'.format(hmm.nstates))\n",
    "    ax.set_xlabel('$\\Phi$')\n",
    "axes[0].set_ylabel('$\\Psi$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... we see that the six state HMM is able to subdivide the two most metastable states of the four state HMM and, thus, give us a more detailed view on the underlying system. As one would have expected from the implied timescale plot, the metastable dynamics is already well-described with $4$ hidden states.\n",
    "\n",
    "Due to the low sensibility to discretization errors, we can afford to estimate HMMs at smaller lagtimes than MSMs and, thus, resolve  more processes.\n",
    "\n",
    "Like with classical MSMs, we can further analyze properties of the HMM. As an example, have a look at the transition paths and committor probabilities below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [0]\n",
    "B = [3]\n",
    "flux = pyemma.msm.tpt(hmm_4, A, B)\n",
    "\n",
    "highest_membership = hmm_4.metastable_distributions.argmax(1)\n",
    "coarse_state_centers = cluster.clustercenters[hmm_4.observable_set[highest_membership]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note one important difference when operating on metastable sets: Since HMMs operate directly on the metastable sets, we need not compute the flux between the `msm.metastable_sets` but between the lists of macrostate numbers, e.g. instead of `A = msm.metastable_sets[0]` we need `A = [0]`. \n",
    "\n",
    "Let's now visualize the committor as before. Does it look familiar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "pyemma.plots.plot_contour(\n",
    "    *data_concatenated.T,\n",
    "    flux.committor[hmm_4.metastable_assignments[dtrajs_concatenated]],\n",
    "    cmap='brg',\n",
    "    ax=ax,\n",
    "    mask=True,\n",
    "    cbar_label=r'committor 0 $\\to$ 3',\n",
    "    alpha=0.8,\n",
    "    zorder=-1)\n",
    "\n",
    "pyemma.plots.plot_flux(\n",
    "    flux,\n",
    "    coarse_state_centers,\n",
    "    flux.stationary_distribution,\n",
    "    ax=ax,\n",
    "    show_committor=False,\n",
    "    figpadding=0,\n",
    "    show_frame=True,\n",
    "    arrow_label_format='%2.e / ps');\n",
    "\n",
    "ax.set_xlabel('$\\Phi$')\n",
    "ax.set_ylabel('$\\Psi$')\n",
    "ax.set_xlim(data_concatenated[:, 0].min(), data_concatenated[:, 0].max())\n",
    "ax.set_ylim(data_concatenated[:, 1].min(), data_concatenated[:, 1].max())\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see here, in addition to the properties described here, HMMs provide the same analysis tools as MSMs. E.g. eigenvectors and mean first passage times can be extracted as described in previous notebooks. \n",
    "\n",
    "Let us now repeat this approach again for another featurization: we already know that it is possible to resolve six metastable states (five slow processes) using an HMM estimated on a discretization of the backbone torsions. Can you achieve the same level of resolution using heavy atom distances and a suitable TICA projection?\n",
    "\n",
    "**Exercise 1**: obtain the heavy atom distances, use TICA for dimension reduction, and discretize using a method of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "feat = #FIXME\n",
    "feat. #FIXME\n",
    "data = #FIXME\n",
    "\n",
    "tica = #FIXME\n",
    "tica_concatenated = np.concatenate(tica.get_output())\n",
    "\n",
    "cluster = #FIXME\n",
    "dtrajs_concatenated = #FIXME\n",
    "\n",
    "its = pyemma.msm.its(\n",
    "    cluster.dtrajs, lags=[1, 2, 5, 10, 20, 50], nits=4, errors='bayes')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "pyemma.plots.plot_feature_histograms(tica_concatenated, ax=axes[0])\n",
    "pyemma.plots.plot_density(*tica_concatenated[:, :2].T, ax=axes[1], cbar=False, alpha=0.1)\n",
    "axes[1].scatter(*cluster.clustercenters[:, :2].T, s=15, c='C1')\n",
    "axes[1].set_xlabel('IC 1')\n",
    "axes[1].set_ylabel('IC 2')\n",
    "pyemma.plots.plot_implied_timescales(its, ax=axes[2], units='ps')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "feat = pyemma.coordinates.featurizer(pdb)\n",
    "pairs = feat.pairs(feat.select_Heavy())\n",
    "feat.add_distances(pairs)\n",
    "data = pyemma.coordinates.load(files, features=feat)\n",
    "\n",
    "tica = pyemma.coordinates.tica(data, lag=3)\n",
    "tica_concatenated = np.concatenate(tica.get_output())\n",
    "\n",
    "cluster = pyemma.coordinates.cluster_kmeans(tica, k=75, max_iter=50, stride=10)\n",
    "dtrajs_concatenated = np.concatenate(cluster.dtrajs)\n",
    "\n",
    "its = pyemma.msm.its(\n",
    "    cluster.dtrajs, lags=[1, 2, 5, 10, 20, 50], nits=4, errors='bayes')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "pyemma.plots.plot_feature_histograms(tica_concatenated, ax=axes[0])\n",
    "pyemma.plots.plot_density(*tica_concatenated[:, :2].T, ax=axes[1], cbar=False, alpha=0.1)\n",
    "axes[1].scatter(*cluster.clustercenters[:, :2].T, s=15, c='C1')\n",
    "axes[1].set_xlabel('IC 1')\n",
    "axes[1].set_ylabel('IC 2')\n",
    "pyemma.plots.plot_implied_timescales(its, ax=axes[2], units='ps')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**: let's see if your discretized data is suitable to converge five slow implied timescales using a bayesian HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "pyemma.plots.plot_implied_timescales #FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "pyemma.plots.plot_implied_timescales(\n",
    "    pyemma.msm.timescales_hmsm(\n",
    "        cluster.dtrajs, 6, lags=[1, 2, 3, 4], errors='bayes'), units='ps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**: estimate a bayesian HMM and perform a CK test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "hmm = #FIXME\n",
    "pyemma.plots. #FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "hmm = pyemma.msm.bayesian_hidden_markov_model(cluster.dtrajs, 6, lag=1, dt_traj='1 ps')\n",
    "pyemma.plots.plot_cktest(hmm.cktest(mlags=2), units='ps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**: now that you have a model, be creative and visualize the metastable regions in your projected space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def draw_panel(ax, i, j):\n",
    "    pyemma.plots.plot_state_map(\n",
    "        *tica_concatenated[:, [i, j]].T,\n",
    "        hmm.metastable_assignments[dtrajs_concatenated],\n",
    "        ax=ax)\n",
    "    ax.set_xlabel('IC {}'.format(i + 1))\n",
    "    ax.set_ylabel('IC {}'.format(j + 1))\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "draw_panel(axes[0, 0], 0, 2)\n",
    "draw_panel(axes[0, 1], 1, 2)\n",
    "draw_panel(axes[1, 0], 0, 1)\n",
    "axes[1, 1].set_axis_off()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "In this notebook, we have learned how to use a hidden Markov state model (HMM) and how they differ from an MSM. In detail, we have used\n",
    "- `pyemma.msm.timescales_hmsm()` function to obtain an implied timescale object for HMMs,\n",
    "- `pyemma.msm.estimate_hidden_markov_model()` to estimate a regular HMM,\n",
    "- `pyemma.msm.bayesian_hidden_markov_model()` to estimate a Bayesian HMM, \n",
    "- the `metastable_assignments` attribute of an HMM object to access the metastable membership of discrete states, \n",
    "- the `hidden_state_probabilities` attribute to assess probabilities of hidden states over time, and\n",
    "- the `hidden_state_trajectories` attribute that extracts the most likely trajectory in hidden state space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

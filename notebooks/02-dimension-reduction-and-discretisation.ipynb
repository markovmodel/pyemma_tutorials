{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Dimension reduction and discretization\n",
    "In this notebook, we will cover how to perform dimension reduction and discretization of molecular dynamics data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mdshare\n",
    "import pyemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: preprocessed, two-dimensional data (toy model)\n",
    "We load the two-dimensional trajectory from an archive using `numpy` and visualize the marginal and joint distributions of both components. In order to make the important concept of metastability easier to understand, an excerpt from the original trajectory is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = mdshare.fetch('hmm-doublewell-2d-100k.npz', working_directory='data')\n",
    "with np.load(file) as fh:\n",
    "    data = fh['trajectory']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "pyemma.plots.plot_feature_histograms(data, feature_labels=['$x$', '$y$'], ax=axes[0])\n",
    "for dim in range(2): \n",
    "    axes[0].plot(data[:300, dim], np.linspace(-.2 + dim, .8 + dim, 300), color='C2', alpha=.6)\n",
    "    axes[0].annotate('${}(time)$'.format('x' if dim == 0 else 'y'), xy=(3, .6 + dim), xytext=(3, dim),\n",
    "            arrowprops=dict(fc='C2', ec='None', alpha=.6, width=2))\n",
    "\n",
    "axes[1].scatter(*data.T, s=1, alpha=0.3)\n",
    "axes[1].set_xlabel('$x$')\n",
    "axes[1].set_ylabel('$y$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the low dimensionality of this data set, we can directly attempt to discretize, e.g. with $k$-means with $200$ centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_kmeans = pyemma.coordinates.cluster_kmeans(data, k=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or with a regspace technique where all centers should have a minimal pairwise distance of $0.3$ units of length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_regspace = pyemma.coordinates.cluster_regspace(data, dmin=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and, then, we visualize both sets of centers on top of the joint distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "for ax, cls in zip(axes.flat, [cluster_kmeans, cluster_regspace]):\n",
    "    ax.scatter(*data.T, s=1, alpha=0.3)\n",
    "    ax.scatter(*cls.clustercenters.T, s=15)\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you noticed how the $k$-means centers follow the density of the data points while the regspace centers are spread uniformly over the whole area?\n",
    "\n",
    "The main result of a discretization, however, is not the set of centers but the timeseries of discrete states. These are accessible via the `dtrajs` attribute of any clustering object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster_kmeans.dtrajs)\n",
    "print(cluster_regspace.dtrajs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each trajectory passed to the clustering object, we get a corresponding discrete trajectory.\n",
    "\n",
    "Instead of discretizing the full (two-dimensional) space, we can attempt to find a one-dimensional subspace which\n",
    "1. describes the slow dynamics of the data set equally well but\n",
    "2. is easier to discretize.\n",
    "\n",
    "One widespread method for dimension reduction is the principal component analysis (PCA) which finds a subspace with maximized variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = pyemma.coordinates.pca(data, dim=1)\n",
    "pca_out = pca.get_output()\n",
    "print(pca_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another technique is the time-lagged independent component analysis (TICA) which finds a subspace with maximized autocorrelation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tica = pyemma.coordinates.tica(data, dim=1, lag=1)\n",
    "tica_out = tica.get_output()\n",
    "print(tica_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are many cases where PCA can find a suitable subspace, there are also many cases where the PCA-based subspace neglects the slow dynamics. In our example, the slow process is the jump between both wells along the $y$ axis while the $x$-axis contains only random noise. For both, PCA and TICA, we show the distribution after projecting the full dynamics onto a one-dimensional subspace (left) and the direction of projection (right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "pyemma.plots.plot_feature_histograms(\n",
    "    np.concatenate([pca_out[0], tica_out[0]], axis=1),\n",
    "    feature_labels=['PCA', 'TICA'],\n",
    "    ax=axes[0])\n",
    "axes[1].scatter(*data.T, s=0.1, alpha=0.3, c='grey')\n",
    "axes[1].plot(\n",
    "    [0, 3 * abs(pca.eigenvectors[0, 0])],\n",
    "    [0, 3 * abs(pca.eigenvectors[1, 0])],\n",
    "    linewidth=3,\n",
    "    label='PCA')\n",
    "axes[1].plot(\n",
    "    [0, 3 * abs(tica.eigenvectors[0, 0])],\n",
    "    [0, 3 * abs(tica.eigenvectors[1, 0])],\n",
    "    linewidth=3,\n",
    "    label='TICA')\n",
    "axes[1].set_xlabel('$x$')\n",
    "axes[1].set_ylabel('$y$')\n",
    "axes[1].legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that TICA projects along the $y$-axis and, thus, yields a subspace which clearly resolves both metastable states. PCA on the other hand projects closely along the $x$-axis and does not resolve both metastabel states. This is a case in point where variance maximization does not lead to a subspace which resolves the slow dynamics of the system.\n",
    "\n",
    "This effect can also be seen when we plot the subapce timeseries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "ax.plot(pca_out[0][:300], label='PCA')\n",
    "ax.plot(tica_out[0][:300], label='TICA')\n",
    "ax.set_xlabel('time / steps')\n",
    "ax.set_ylabel('feature values')\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of TICA, we observe that the projected coordinate jumps between two clearly separated plateaus. For PCA, we observe only random fluctuations without any hint of metastablility.\n",
    "\n",
    "For the discretization of both projections, we show an alternative to $k$-means and regspace clustering, the manual creation of a one-dimensional grid with uniform distance between neighbouring centers.\n",
    "\n",
    "We first discretize the PCA projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_pca = np.linspace(np.min(pca_out), np.max(pca_out), 50)\n",
    "dtrajs_pca = pyemma.coordinates.assign_to_centers(pca_out, centers=centers_pca.reshape(-1, 1))\n",
    "print(dtrajs_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then the TICA projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_tica = np.linspace(np.min(tica_out), np.max(tica_out), 50)\n",
    "dtrajs_tica = pyemma.coordinates.assign_to_centers(tica_out, centers=centers_tica.reshape(-1, 1))\n",
    "print(dtrajs_tica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and, finally, we visualize the distributions of discrete states for both projections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pyemma.plots.plot_feature_histograms(\n",
    "    np.concatenate([dtrajs_pca, dtrajs_tica]).T,\n",
    "    feature_labels=['PCA', 'TICA'])\n",
    "ax.set_xlabel('discrete state');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can see that the TICA-based discrete trajectory resolves the two metastable states whereas the PCA-based discrete trajectory does not resolve these states.\n",
    "\n",
    "## Case 2: low-dimensional molecular dynamics data (alanine dipeptide)\n",
    "We fetch the alanine dipeptide data set, load the backbone torsions into memory, and visualize the margial and joint distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb = mdshare.fetch('alanine-dipeptide-nowater.pdb', working_directory='data')\n",
    "files = mdshare.fetch('alanine-dipeptide-*-250ns-nowater.dcd', working_directory='data')\n",
    "\n",
    "feat = pyemma.coordinates.featurizer(pdb)\n",
    "feat.add_backbone_torsions()\n",
    "data = pyemma.coordinates.load(files, features=feat)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "pyemma.plots.plot_feature_histograms(np.concatenate(data), feature_labels=['$\\Phi$', '$\\Psi$'], ax=axes[0])\n",
    "axes[1].scatter(*np.concatenate(data).T, s=1, alpha=0.3)\n",
    "axes[1].set_xlabel('$\\Phi$')\n",
    "axes[1].set_ylabel('$\\Psi$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the previous example, we perform a $k$-means ($200$ centers) and a regspace ($0.3$ radians center distance) on the full two-dimensional data set and visualize the obtained centers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_kmeans = pyemma.coordinates.cluster_kmeans(data, k=200)\n",
    "cluster_regspace = pyemma.coordinates.cluster_regspace(data, dmin=0.3)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "for ax, cls in zip(axes.flat, [cluster_kmeans, cluster_regspace]):\n",
    "    ax.scatter(*np.concatenate(data).T, s=1, alpha=0.3)\n",
    "    ax.scatter(*cls.clustercenters.T, s=15)\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, notice the difference between $k$-means and regspace clustering.\n",
    "\n",
    "Now, we use a different featurization for the same data set and revisit how to use PCA and TICA.\n",
    "\n",
    "**Exercise**: Load the heavy atoms' positions into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = pyemma.coordinates.featurizer(pdb)\n",
    "feat.add_selection(feat.select_Heavy())\n",
    "data = pyemma.coordinates.load(files, features=feat)\n",
    "\n",
    "print('We have %d features.' % feat.dimension())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "pyemma.plots.plot_feature_histograms(np.concatenate(data), feature_labels=feat.describe(), ax=ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretizing a $30$-dimensional feature space is impractical. Let's use PCA to find a low-dimensional projection and visualize the marginal distributions of all principal components (PCs) as well as the joint distributions for the first two PCs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = pyemma.coordinates.pca(data)\n",
    "pca_all = np.concatenate(pca.get_output())\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "pyemma.plots.plot_feature_histograms(\n",
    "    pca_all, ['PC %d' % (i + 1) for i in range(pca.dimension())], ax=axes[0])\n",
    "axes[1].scatter(*pca_all[:, :2].T, s=1, alpha=0.3)\n",
    "pyemma.plots.plot_free_energy(*pca_all[:, :2].T, ax=axes[2])\n",
    "for ax in axes.flat[1:]:\n",
    "    ax.set_xlabel('PC 1')\n",
    "    ax.set_ylabel('PC 2')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the default parameters, PCA will return as many dimensions as necessary to explain $95\\%$ of the variance; in this case, we have found a five-dimensional subspace which does seem to resolve some metastability in the first three principal components.\n",
    "\n",
    "**Exercise**: apply TICA and visualize the marginal distributions of all independent components (ICs) as well as the joint distributions of the first two ICs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tica = pyemma.coordinates.tica(data)\n",
    "tica_all = np.concatenate(tica.get_output())\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "pyemma.plots.plot_feature_histograms(\n",
    "    tica_all, ['IC %d' % (i + 1) for i in range(tica.dimension())], ax=axes[0])\n",
    "axes[1].scatter(*tica_all[:, :2].T, s=1, alpha=0.3)\n",
    "pyemma.plots.plot_free_energy(*tica_all[:, :2].T, ax=axes[2])\n",
    "for ax in axes.flat[1:]:\n",
    "    ax.set_xlabel('IC 1')\n",
    "    ax.set_ylabel('IC 2')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TICA, by default, uses a lag time of $10$ steps and a kinetic variance cutoff of $95\\%$ to determine the number of ICs. We observe that this projection does resolve some metastability in both ICs.\n",
    "\n",
    "Whether these projections are suitable for building Markov state models, though, remains to be seen in later tests.\n",
    "\n",
    "**Exercise**: Perform PCA on the heavy atoms' positions data set with a target dimension of two; then discretize the two-dimensional subspace using $k$-means with $200$ centers.\n",
    "\n",
    "**Hint:** Look up the parameters of `pyemma.coordinates.pca()`, especially the `dim` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = pyemma.coordinates.pca(data, dim=2)\n",
    "pca_all = np.concatenate(pca.get_output())\n",
    "\n",
    "cluster = pyemma.coordinates.cluster_kmeans(pca, k=200, max_iter=50)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "pyemma.plots.plot_feature_histograms(\n",
    "    pca_all, ['PC %d' % (i + 1) for i in range(pca.dimension())], ax=axes[0])\n",
    "axes[1].scatter(*pca_all.T, s=1, alpha=0.3)\n",
    "axes[1].scatter(*cluster.clustercenters.T, s=15)\n",
    "axes[1].set_xlabel('PC 1')\n",
    "axes[1].set_ylabel('PC 2')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Perform TICA at lag time $1$ step on the heavy atoms' positions data set with a target dimension of two; then discretize the two-dimensional subspace using $k$-means with $200$ centers.\n",
    "\n",
    "**Hint:** Look up the parameters of `pyemma.coordinates.tica()`, especially the `dim` and `lag` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tica = pyemma.coordinates.tica(data, lag=1, dim=2)\n",
    "tica_all = np.concatenate(tica.get_output())\n",
    "\n",
    "cluster = pyemma.coordinates.cluster_kmeans(tica, k=200, max_iter=50)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "pyemma.plots.plot_feature_histograms(\n",
    "    tica_all, ['IC %d' % (i + 1) for i in range(tica.dimension())], ax=axes[0])\n",
    "axes[1].scatter(*tica_all.T, s=1, alpha=0.3)\n",
    "axes[1].scatter(*cluster.clustercenters.T, s=15)\n",
    "axes[1].set_xlabel('IC 1')\n",
    "axes[1].set_ylabel('IC 2')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you noticed the difference in the first two ICs for lag times $10$ steps vs $1$ step?\n",
    "\n",
    "## Case 3: another molecular dynamics data set (pentapeptide)\n",
    "\n",
    "**Exercise**: Fetch the pentapeptide data set, load the cossin transformations of the backbone and $\\chi_1$ sidechain torsions into memory, and visualize the margial distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb = mdshare.fetch('pentapeptide-impl-solv.pdb', working_directory='data')\n",
    "files = mdshare.fetch('pentapeptide-*-500ns-impl-solv.xtc', working_directory='data')\n",
    "\n",
    "feat = pyemma.coordinates.featurizer(pdb)\n",
    "feat.add_backbone_torsions(cossin=True)\n",
    "feat.add_sidechain_torsions(which='chi1', cossin=True)\n",
    "\n",
    "data = pyemma.coordinates.load(files, features=feat)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "pyemma.plots.plot_feature_histograms(np.concatenate(data), feature_labels=feat.describe(), ax=ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Perform PCA with default parameters and visualize the marginal distributions of all PCs and the joint distributions of the first two PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = pyemma.coordinates.pca(data)\n",
    "pca_all = np.concatenate(pca.get_output())\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "pyemma.plots.plot_feature_histograms(\n",
    "    pca_all, ['PC %d' % (i + 1) for i in range(pca.dimension())], ax=axes[0])\n",
    "axes[1].scatter(*pca_all[:, :2].T, s=1, alpha=0.3)\n",
    "pyemma.plots.plot_free_energy(*pca_all[:, :2].T, ax=axes[2])\n",
    "for ax in axes.flat[1:]:\n",
    "    ax.set_xlabel('PC 1')\n",
    "    ax.set_ylabel('PC 2')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Perform TICA with the lag times given below and, for each lag time, visualize the marginal distributions of all ICs and the joint distributions of the first two ICs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [1, 2, 5, 10, 20, 50]\n",
    "\n",
    "fig, axes = plt.subplots(len(lags), 3, figsize=(10, len(lags) * 3))\n",
    "for i, lag in enumerate(lags):\n",
    "    tica = pyemma.coordinates.tica(data, lag=lag)\n",
    "    tica_all = np.concatenate(tica.get_output())\n",
    "    pyemma.plots.plot_feature_histograms(\n",
    "        tica_all, ['IC %d' % (i + 1) for i in range(tica.dimension())], ax=axes[i, 0])\n",
    "    axes[i, 0].set_title('lag time = %d steps' % lag)\n",
    "    axes[i, 1].scatter(*tica_all[:, :2].T, s=1, alpha=0.3)\n",
    "    pyemma.plots.plot_free_energy(*tica_all[:, :2].T, ax=axes[i, 2], cbar=False)\n",
    "for ax in axes[:, 1:].flat:\n",
    "    ax.set_xlabel('IC 1')\n",
    "    ax.set_ylabel('IC 2')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you noticed that increasing the lag time\n",
    "1. leads to a rotation of the projection and\n",
    "2. reduces the number of ICs to explain $95\\%$ (default) of the kinetic variance?\n",
    "\n",
    "Note that, while we can get lower and lower dimensional subspaces with increased lag times, we also loose information from the faster processes.\n",
    "\n",
    "How to choose the optimal lag time for a TICA projection often is a hard problem, but we will learn systematic approaches in later notebooks.\n",
    "\n",
    "**Exercise**: Perform PCA with target dimension $3$ on the current feature set and discretize the projected space using $k$-means with $200$ centers and a stride of $5$ to reduce the computational effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = pyemma.coordinates.pca(data, dim=3)\n",
    "pca_all = np.concatenate(pca.get_output(stride=5))\n",
    "\n",
    "cluster = pyemma.coordinates.cluster_kmeans(pca, k=200, max_iter=50, stride=5)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "pyemma.plots.plot_feature_histograms(\n",
    "    pca_all, ['PC %d' % (i + 1) for i in range(pca.dimension())], ax=axes[0, 0])\n",
    "for ax, (i, j) in zip(axes.flat[1:], [[0, 1], [0, 2], [1, 2]]):\n",
    "    ax.scatter(*pca_all[:, [i, j]].T, s=1, alpha=0.3)\n",
    "    ax.scatter(*cluster.clustercenters[:, [i, j]].T, s=15)\n",
    "    ax.set_xlabel('PC %d' % (i + 1))\n",
    "    ax.set_ylabel('PC %d' % (j + 1))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Perform TICA with target dimension $3$ and lag time $20$ steps on the curent feature set and discretize the projected space using $k$-means with $200$ centers and a stride of $5$ to reduce the computational effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tica = pyemma.coordinates.tica(data, lag=20, dim=3)\n",
    "tica_all = np.concatenate(tica.get_output(stride=5))\n",
    "\n",
    "cluster = pyemma.coordinates.cluster_kmeans(tica, k=200, max_iter=50, stride=5)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "pyemma.plots.plot_feature_histograms(\n",
    "    tica_all, ['IC %d' % (i + 1) for i in range(tica.dimension())], ax=axes[0, 0])\n",
    "for ax, (i, j) in zip(axes.flat[1:], [[0, 1], [0, 2], [1, 2]]):\n",
    "    ax.scatter(*tica_all[:, [i, j]].T, s=1, alpha=0.3)\n",
    "    ax.scatter(*cluster.clustercenters[:, [i, j]].T, s=15)\n",
    "    ax.set_xlabel('IC %d' % (i + 1))\n",
    "    ax.set_ylabel('IC %d' % (j + 1))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "In this notebook, we have learned how to reduce the dimension of molecular simulation data and discretize the projected dynamics with `pyemma`. In detail, we have used\n",
    "- `pyemma.coordinates.pca()` to perform a principal components analysis,\n",
    "- `pyemma.coordinates.tica()` to perform a time-lagged independent component analysis, and\n",
    "- `pyemma.coordinates.cluster_kmeans()` to perform a $k$-means clustering,\n",
    "- `pyemma.coordinates.cluster_regspace()` to perform a regspace clustering, and\n",
    "- `pyemma.coordinates.assign_to_centers()` to map trajectories to user-defined centers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyemma_tutorial",
   "language": "python",
   "name": "pyemma_tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
